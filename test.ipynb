{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc545afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm():\n",
    "    from llama_index.llms.google_genai import GoogleGenAI\n",
    "    try:\n",
    "        gemini_llm = GoogleGenAI(model=\"gemini-2.0-flash\")\n",
    "        # Thử gọi một request nhỏ để kiểm tra rate limit\n",
    "        gemini_llm.complete(\"test\")\n",
    "        return gemini_llm\n",
    "    except Exception as e:\n",
    "        print(f\"Falling back to Ollama due to: {str(e)}\")\n",
    "        from llama_index.llms.ollama import Ollama\n",
    "        return Ollama(model=\"qwen2.5:0.5b\", request_timeout=120.0)\n",
    "\n",
    "def get_embed_model(model_path=\"bkai-foundation-models/vietnamese-bi-encoder\"):\n",
    "    from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "    return HuggingFaceEmbedding(\n",
    "        model_name=model_path\n",
    "    )\n",
    "    \n",
    "    \n",
    "def get_index(local = True, _embed_model = None):\n",
    "    import os\n",
    "    from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
    "    from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "    import qdrant_client\n",
    "    qdrant_collection_name = \"local_law_documents\" if local else \"law_db\"\n",
    "    _embed_model = _embed_model if _embed_model else get_embed_model(model_path=\"bkai-foundation-models/vietnamese-bi-encoder\")\n",
    "    loaded_index = VectorStoreIndex.from_vector_store(\n",
    "        QdrantVectorStore(\n",
    "            client=qdrant_client.QdrantClient(\n",
    "                \"https://08838c4e-e0ad-488e-a2a9-b217fa55c19a.us-east-1-0.aws.cloud.qdrant.io\",\n",
    "                api_key=os.environ[\"QDRANT_API_KEY\"],\n",
    "            ) if not local else qdrant_client.QdrantClient(\n",
    "                \"http://localhost:6333\",\n",
    "            ),\n",
    "            collection_name=qdrant_collection_name,\n",
    "            enable_hybrid=True,\n",
    "            fastembed_sparse_model=\"Qdrant/bm25\",\n",
    "            batch_size=20,\n",
    "        ),\n",
    "        embed_model=_embed_model,\n",
    "    )\n",
    "    return loaded_index\n",
    "    \n",
    "def get_query_engine(loaded_index=None,llm=None,top_k_sparse=10,top_k_similarity=2):\n",
    "    \n",
    "    from llama_index.core.prompts import RichPromptTemplate\n",
    "    from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "    \n",
    "    loaded_index = loaded_index if loaded_index else get_index(local=True)\n",
    "    llm = llm if llm else get_llm()\n",
    "    # Postprocessor: Cutoff node if similarity is under threshold\n",
    "    sim_postprocessor = SimilarityPostprocessor(similarity_cutoff=0.4)\n",
    "    qa_prompt_tmpl_str = (\n",
    "        \"Bạn là trợ lý tư vấn pháp luật cho nhiệm vụ hỏi đáp với người dùng.\\n\"\n",
    "        \"Sử dụng các phần sau của bối cảnh được truy xuất để trả lời câu hỏi.\\n\"\n",
    "        \"Nếu bạn không biết câu trả lời, đừng cố tạo câu trả lời..\\n\"\n",
    "        \"Ngữ cảnh cung cấp:\\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"{{ context_str }}\\n\"\n",
    "        \"Hãy trả lời câu hỏi sau với phong cách của một luật sư.\\n\"\n",
    "        \"Người dùng hỏi: {{ query_str }}\\n\"\n",
    "        \"Trả lời: \"\n",
    "    )\n",
    "    qa_prompt_tmpl = RichPromptTemplate(qa_prompt_tmpl_str)\n",
    "    \n",
    "    query_engine = loaded_index.as_query_engine(\n",
    "        text_qa_template=qa_prompt_tmpl,\n",
    "        llm=llm,\n",
    "        similarity_top_k=top_k_similarity,\n",
    "        sparse_top_k=top_k_sparse,\n",
    "        vector_store_query_mode=\"hybrid\",\n",
    "        node_postprocessors=[sim_postprocessor],\n",
    "    )\n",
    "    return query_engine\n",
    "\n",
    "def get_chat_engine(query_engine=None,llm=None):\n",
    "    from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "    from llama_index.core import PromptTemplate\n",
    "    from llama_index.core.llms import ChatMessage, MessageRole\n",
    "    llm = llm if llm else get_llm()\n",
    "    loaded_index = get_index(local=True)\n",
    "    query_engine = query_engine if query_engine else get_query_engine(loaded_index,llm)\n",
    "    \n",
    "    custom_prompt =\"\"\"\\\n",
    "    Cho đoạn hội thoại(Giữa người dùng và trợ lý) và một câu hỏi tiếp theo từ người dùng, \\\n",
    "    vui lòng viết lại câu hỏi để nó trở thành một câu hỏi độc lập, \\\n",
    "    có thể hiểu được toàn bộ ngữ cảnh đoạn hội thoại. \\\n",
    "\n",
    "    <Đoạn hội thoại>\n",
    "    {chat_history}\n",
    "\n",
    "    <Câu hỏi tiếp theo>\n",
    "    {question}\n",
    "\n",
    "    <Câu hỏi độc lập>\n",
    "    \"\"\"\n",
    "    \n",
    "    custom_chat_history = [\n",
    "        ChatMessage(\n",
    "            role=MessageRole.USER,\n",
    "            content=\"Chào bạn, tôi cần sự giúp đỡ của bạn về một vấn đề pháp lý, lĩnh vực hôn nhân gia đình.\",\n",
    "        ),\n",
    "        ChatMessage(\n",
    "            role=MessageRole.ASSISTANT, content=\"Được thôi! Tôi có thể giúp gì cho bạn?\"\n",
    "        ),\n",
    "    ]\n",
    "    chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "        query_engine=query_engine,\n",
    "        condense_question_prompt= PromptTemplate(custom_prompt),\n",
    "        chat_history=custom_chat_history,\n",
    "        verbose=True,\n",
    "        llm=llm,\n",
    "    )\n",
    "    return chat_engine\n",
    "\n",
    "def init_chat_engine():  \n",
    "    import os  \n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    \n",
    "    import huggingface_hub\n",
    "    huggingface_hub.login(token=os.environ[\"HUGGINGFACE_TOKEN\"])\n",
    "    chat_engine = get_chat_engine()\n",
    "    return chat_engine\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ff19543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying with: Tôi muốn hỏi, nếu chồng tôi có hành vi bạo hành đối với mẹ con tôi thì tôi có đủ điều kiện để ly hôn theo quy định của pháp luật không?\n",
      "\n",
      "Chào bạn,\n",
      "\n",
      "Để trả lời câu hỏi của bạn về việc liệu hành vi bạo hành của chồng bạn đối với mẹ con bạn có đủ điều kiện để ly hôn theo quy định của pháp luật hay không, tôi xin đưa ra ý kiến tư vấn dựa trên các quy định hiện hành như sau:\n",
      "\n",
      "**Căn cứ pháp lý:**\n",
      "\n",
      "*   **Luật Hôn nhân và Gia đình:** Mặc dù bạn không cung cấp thông tin cụ thể về việc ly hôn đơn phương hay thuận tình, nhưng hành vi bạo hành có thể là căn cứ để bạn yêu cầu ly hôn đơn phương.\n",
      "\n",
      "**Phân tích:**\n",
      "\n",
      "1.  **Ly hôn đơn phương:** Nếu chồng bạn có hành vi bạo hành đối với bạn và con bạn, đây có thể được xem là một trong những căn cứ để bạn yêu cầu ly hôn đơn phương. Tòa án sẽ xem xét mức độ nghiêm trọng của hành vi bạo hành, ảnh hưởng của nó đến tinh thần và thể chất của bạn và con bạn để đưa ra quyết định.\n",
      "\n",
      "**Lời khuyên:**\n",
      "\n",
      "Để có thể đưa ra ý kiến tư vấn chính xác và đầy đủ hơn, tôi cần thêm thông tin chi tiết về tình huống của bạn, ví dụ như:\n",
      "\n",
      "*   Bạn muốn ly hôn đơn phương hay thuận tình?\n",
      "*   Bạn có bằng chứng về hành vi bạo hành của chồng bạn không (ví dụ: hình ảnh, video, giấy chứng nhận thương tích, lời khai của nhân chứng)?\n",
      "*   Bạn có yêu cầu gì về việc chia tài sản, quyền nuôi con sau khi ly hôn không?\n",
      "\n",
      "Bạn nên thu thập đầy đủ chứng cứ về hành vi bạo hành (nếu có) và liên hệ với luật sư để được tư vấn cụ thể hơn về quy trình và thủ tục ly hôn trong trường hợp của bạn.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !docker run -p 6333:6333 -v qdrant_data:/qdrant/storage qdrant/qdrant\n",
    "\n",
    "llm = get_llm()\n",
    "embed_model = get_embed_model()\n",
    "loaded_index = get_index(local=True, _embed_model=embed_model)\n",
    "query_engine = get_query_engine(loaded_index,llm)\n",
    "chat_engine = get_chat_engine(query_engine,llm)\n",
    "response = chat_engine.chat(\"Chồng tôi bạo hành mẹ con tôi thì tôi có được ly hôn không\")\n",
    "print(response.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
