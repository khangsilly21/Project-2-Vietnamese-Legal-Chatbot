{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e79f79d2",
   "metadata": {},
   "source": [
    "## 1. LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240431d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "import re\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "reader = SimpleDirectoryReader('data/corpus',required_exts=[\".docx\"])\n",
    "documents = reader.load_data()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cb2f33",
   "metadata": {},
   "source": [
    "Get metadata info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddd8eb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from llama_index.core import Document\n",
    "def make_doc(doc: Document, id: int) -> Document:\n",
    "    #NOTE: Xoá header của doc\n",
    "    text = doc.text\n",
    "    pattern = r\"(NGHỊ QUYẾT|QUYẾT ĐỊNH|HƯỚNG DẪN|NGHỊ ĐỊNH|THÔNG TƯ|PHÁP LỆNH|LUẬT|KẾ HOẠCH|CHỈ THỊ).*\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        #NOTE: Lấy nội dung sau header\n",
    "        doc_content = text[match.start():]\n",
    "\n",
    "        #NOTE: Lấy nội dung trước header\n",
    "        doc_header = text[:match.start()]\n",
    "        \n",
    "        #NOTE: Trích xuất thông tin\n",
    "        number_pattern = r\"(?:số|Số):\\s*([^\\s]+)\"\n",
    "        number_match = re.search(number_pattern, doc_header)\n",
    "\n",
    "        \n",
    "        #NOTE: Trích xuất tiêu đề\n",
    "        title_pattern = r\"(NGHỊ QUYẾT|QUYẾT ĐỊNH|HƯỚNG DẪN|NGHỊ ĐỊNH|THÔNG TƯ|PHÁP LỆNH|LUẬT|KẾ HOẠCH|CHỈ THỊ)\\s+(.*?)\\s+_{2,}\"\n",
    "        title_match = re.search(title_pattern, doc_content, re.DOTALL)\n",
    "        \n",
    "        return Document(\n",
    "            text=doc_content.strip(),\n",
    "            doc_id = f\"doc_{id}\",\n",
    "            metadata={\n",
    "                \"file_path\": doc.metadata[\"file_path\"],\n",
    "                \"type\" : match.group(1).strip() if match else None,\n",
    "                \"number\": number_match.group(1).strip() if number_match else None,\n",
    "                \"title\": title_match.group(2).strip() if title_match else None,  \n",
    "            },\n",
    "            \n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Không hỗ trợ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b01e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_meta_docs = []\n",
    "for id,doc in enumerate(documents):\n",
    "    try:\n",
    "        new_doc = make_doc(doc=doc, id = id)\n",
    "        new_doc.excluded_llm_metadata_keys = [\"file_path\"]\n",
    "        new_doc.excluded_embed_metadata_keys = [\"file_path\"]\n",
    "        extracted_meta_docs.append(new_doc)\n",
    "    except ValueError as e:\n",
    "        print(f\"Document {id} không hỗ trợ: {e}\")\n",
    "\n",
    "len(extracted_meta_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1356dc79",
   "metadata": {},
   "source": [
    "## CHUNKING BY REGEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f4f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from llama_index.core.node_parser import TextSplitter\n",
    "from llama_index.core.schema import TextNode, NodeRelationship, RelatedNodeInfo\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class HierarchicalRegexNodeParser(TextSplitter):\n",
    "    def __init__(self, chunk_size=512, chunk_overlap=50, max_token_limit=512):\n",
    "        super().__init__(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        self._max_token_limit = max_token_limit\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained('bkai-foundation-models/vietnamese-bi-encoder')\n",
    "        self._sentence_splitter = SentenceSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "\n",
    "    def count_tokens(self, text):\n",
    "        \"\"\"Đếm số token của văn bản.\"\"\"\n",
    "        return len(self._tokenizer.encode(text))\n",
    "\n",
    "    def split_text_by_regex(self, text, regex_pattern):\n",
    "\n",
    "        # Tìm tất cả các vị trí khớp với pattern\n",
    "        matches = list(re.finditer(regex_pattern, text))\n",
    "        \n",
    "        # Nếu không có pattern nào khớp, trả về text gốc\n",
    "        if not matches:\n",
    "            return [text.strip()] if text.strip() else []\n",
    "        \n",
    "        chunks = []\n",
    "        # Xử lý phần đầu tiên trước pattern đầu tiên\n",
    "        if matches[0].start() > 0:\n",
    "            chunks.append(text[:matches[0].start()].strip())\n",
    "        \n",
    "        # Xử lý các phần giữa các pattern\n",
    "        for i in range(len(matches)):\n",
    "            # Xác định vị trí bắt đầu của phần hiện tại\n",
    "            start_pos = matches[i].start()\n",
    "            \n",
    "            # Xác định vị trí kết thúc (là vị trí bắt đầu của pattern tiếp theo hoặc cuối văn bản)\n",
    "            end_pos = matches[i+1].start() if i < len(matches) - 1 else len(text)\n",
    "            \n",
    "            # Trích xuất phần text bao gồm pattern hiện tại và nội dung sau nó đến pattern tiếp theo\n",
    "            chunk = text[start_pos:end_pos].strip()\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        return [chunk for chunk in chunks if chunk.strip()]\n",
    "\n",
    "    def split_text(self, text: str) -> list:\n",
    "        \"\"\"Chia văn bản thành các chunk nhỏ hơn với metadata.\"\"\"\n",
    "        # Chia văn bản thành các câu với metadata\n",
    "        sentences = self._sentence_splitter.split_text(text)\n",
    "        chunks=[]\n",
    "        for sentence in sentences:\n",
    "            chunks.append(sentence.strip())\n",
    "        return chunks\n",
    "\n",
    "    def get_base_nodes_from_documents(self, documents):\n",
    "        \"\"\"Chuyển Document thành Node với metadata.\"\"\"\n",
    "        nodes = []\n",
    "        i = 0\n",
    "        for doc in documents:\n",
    "            if doc.metadata['type'] in [\"NGHỊ QUYẾT\", \"NGHỊ ĐỊNH\", \"THÔNG TƯ\", \"PHÁP LỆNH\", \"LUẬT\"]:\n",
    "                # Chia văn bản thành các chunk với metadata\n",
    "                chunks = self.split_text_by_regex(doc.text, r\"\\n\\nĐiều\\s([1-9][0-9]{0,2})\\. \")\n",
    "                \n",
    "                for  chunk in chunks:\n",
    "                    node = TextNode(\n",
    "                        text=chunk,\n",
    "                        id_=f\"{doc.id_}_node_{i}\",\n",
    "                        metadata=doc.metadata.copy()\n",
    "                    )\n",
    "                    nodes.append(node)\n",
    "                    i+=1\n",
    "            else: \n",
    "                chunks = self.split_text(doc.text)\n",
    "                \n",
    "                for chunk in chunks:\n",
    "                    node = TextNode(\n",
    "                        text=chunk,\n",
    "                        id_=f\"{doc.id_}_node_{i}\",\n",
    "                        metadata=doc.metadata.copy()\n",
    "                    )\n",
    "                    nodes.append(node)\n",
    "                    i+=1\n",
    "                \n",
    "        return nodes\n",
    "    \n",
    "    def get_chunks_from_documents(self,documents):\n",
    "        base_nodes = self.get_base_nodes_from_documents(documents)\n",
    "       \n",
    "        for node in base_nodes:\n",
    "            if len(node.text) > self._max_token_limit:\n",
    "                chunks = self.split_text(node.text)\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    new_node = TextNode( \n",
    "                        text=chunk,\n",
    "                        id_=f\"{node.id_}_chunk_{i}\",\n",
    "                        metadata=node.metadata.copy()\n",
    "                    )\n",
    "                    yield new_node\n",
    "            else:\n",
    "                yield node\n",
    "            \n",
    "    # a) : r\"^[a-zA-Z]\\) \"\n",
    "    # 1. : , r\"^[0-9]+\\. \",\n",
    "node_parser = HierarchicalRegexNodeParser()\n",
    "base_nodes = node_parser.get_base_nodes_from_documents(extracted_meta_docs)\n",
    "print('Sô lượng base node:' ,len(base_nodes))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbc56a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(node_parser.get_chunks_from_documents(extracted_meta_docs))\n",
    "print(\"Số lượng nodes: \",len(nodes))\n",
    "print(\"Ví dụ chunk:\")\n",
    "print(nodes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89663b7",
   "metadata": {},
   "source": [
    "## Data Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1b5db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_distribution(lens):\n",
    "    # Thiết lập style\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    sns.set_context(\"paper\")\n",
    "\n",
    "    # Tạo figure với kích thước phù hợp\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Vẽ histogram\n",
    "    n, bins, patches = plt.hist(lens, bins='auto', alpha=0.7, color='steelblue', \n",
    "                            edgecolor='black', linewidth=1.2)\n",
    "\n",
    "    # Thêm KDE (Kernel Density Estimation) để thấy rõ hơn phân phối\n",
    "    sns.kdeplot(lens, color='darkred', linewidth=2)\n",
    "\n",
    "    # Tính toán các thông số thống kê\n",
    "    mean_value = np.mean(lens)\n",
    "    median_value = np.median(lens)\n",
    "    min_value = np.min(lens)\n",
    "    max_value = np.max(lens)\n",
    "\n",
    "    # Vẽ các đường thẳng đứng cho giá trị trung bình và trung vị\n",
    "    plt.axvline(mean_value, color='red', linestyle='--', linewidth=1.5, \n",
    "                label=f'Mean: {mean_value:.2f}')\n",
    "    plt.axvline(median_value, color='green', linestyle='-.', linewidth=1.5, \n",
    "                label=f'Median: {median_value:.2f}')\n",
    "\n",
    "    # Thêm các chú thích và tiêu đề\n",
    "    plt.title('Phân phối độ dài chunks', fontsize=16)\n",
    "    plt.xlabel('Độ dài (số ký tự/token)', fontsize=12)\n",
    "    plt.ylabel('Tần suất', fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    # Thêm textbox thông tin\n",
    "    info_text = f'Min: {min_value}\\nMax: {max_value}\\nMean: {mean_value:.2f}\\nMedian: {median_value:.2f}'\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    plt.text(0.05, 0.95, info_text, transform=plt.gca().transAxes, fontsize=9,\n",
    "            verticalalignment='top', bbox=props)\n",
    "\n",
    "    # Hiển thị biểu đồ\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    #plt.savefig(f'_distribution.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba46a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_base_nodes = []\n",
    "for node in base_nodes:\n",
    "    len_base_nodes.append(len(node.text))\n",
    "len_base_nodes.sort(reverse=True)\n",
    "print('10 base node dài nhất',len_base_nodes[:10])\n",
    "plot_distribution(len_base_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833e3364",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_nodes = []\n",
    "\n",
    "for node in nodes:\n",
    "    len_nodes.append(len(node.text))\n",
    "len_nodes.sort(reverse=True)\n",
    "len_nodes[:10]\n",
    "plot_distribution(len_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bf2c06",
   "metadata": {},
   "source": [
    "## Xử lý ngoại lệ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9f2d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NODE: XÓA FILE dạng điền form\n",
    "# long_nodes = []\n",
    "# for node in nodes:\n",
    "#     if len(node.text) >1200 and \"……\" in node.text:\n",
    "#         long_nodes.append(node)\n",
    "# print(len(long_nodes))\n",
    "# long_nodes[:10]\n",
    "# import os\n",
    "# for node in long_nodes:\n",
    "#     if os.path.exists(node.metadata[\"file_path\"]):\n",
    "#         # Xóa file nếu tồn tại\n",
    "#         os.remove(node.metadata[\"file_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd82a7a6",
   "metadata": {},
   "source": [
    "## Save Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b37c4e",
   "metadata": {},
   "source": [
    "Save into Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a815efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "chunk_data = []\n",
    "i = 1\n",
    "for node in nodes:\n",
    "    chunk_data.append({\n",
    "        \"id\": i,\n",
    "        \"text\": node.text,\n",
    "        \"metadata\": {k: v for k, v in node.metadata.items() if k != \"file_path\"}\n",
    "    })\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "dataset = Dataset.from_list(chunk_data)\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "#login(token=\"your_token\")\n",
    "#dataset.push_to_hub(\"khanglt0004/vietnamese_legal_chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4934f4ad",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2720dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from llama_index.core.schema import TextNode\n",
    "nodes = []\n",
    "def _load_dataset(path=\"khanglt0004/vietnamese_legal_chunks\"):\n",
    "    dataset = load_dataset(path)\n",
    "\n",
    "    \n",
    "    for item in dataset['train']:\n",
    "        new_node = TextNode(\n",
    "            text=item['text'],\n",
    "            id_=str(item['id']),\n",
    "            metadata=item['metadata']\n",
    "        )\n",
    "        nodes.append(new_node)\n",
    "    print(\"Đã tải dữ liệu các chunks, số lượng: \", len(nodes))\n",
    "_load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf8360e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [len(node.text) for node in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f638589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Phân loại theo các mốc\n",
    "bins = [0, 500, 750, 1000, float('inf')]\n",
    "labels = ['<500', '500-750', '750-1000', '>1000']\n",
    "categories = pd.cut(sizes, bins=bins, labels=labels, right=True)\n",
    "\n",
    "# Tính toán số lượng và phần trăm\n",
    "summary = categories.value_counts().sort_index()\n",
    "percentages = (summary / len(sizes) * 100).round(2)\n",
    "\n",
    "# Tạo DataFrame thống kê\n",
    "df = pd.DataFrame({\n",
    "    'Khoảng': labels,\n",
    "    'Số lượng': summary.values,\n",
    "    'Phần trăm': percentages.values\n",
    "})\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(df['Khoảng'], df['Số lượng'], color='skyblue')\n",
    "plt.title('Phân bố độ dài trong các mốc')\n",
    "plt.xlabel('Khoảng độ dài')\n",
    "plt.ylabel('Số lượng')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Hiển thị phần trăm trên cột\n",
    "for i, (count, pct) in enumerate(zip(df['Số lượng'], df['Phần trăm'])):\n",
    "    plt.text(i, count + 0.5, f'{pct}%', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd1d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('chunk_sizes_distribution.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be5296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\n",
    "\n",
    "\n",
    "# model = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder')\n",
    "# for node in nodes:\n",
    "#     if len(node.text) > 1000:\n",
    "#         embeddings = model.encode(nodes[1].text)\n",
    "#         print('embedding:',embeddings)\n",
    "#         break\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
